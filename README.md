# 🧠 인공지능 탐구 과정 요약 (TA Partners 인턴십 발표 기반)

> 인공지능의 역사와 기술적 발전을 되짚으며, 현재 AI 산업이 직면한 한계와 미래 방향을 고찰한 탐구 기반 발표 프로젝트입니다.

---

## 📌 프로젝트 개요

본 발표는 인공지능 기술의 출발점부터 최근 생성형 AI와 sLM(small Language Model)의 대두까지, 기술적 맥락과 시대적 사건들을 연대기적으로 분석한 자료입니다. 발표자는 직접 인공지능 모델의 한계와 사회적 반응, 그리고 차세대 기술의 방향성에 대해 사례 기반 설명을 덧붙였습니다.

---

## 🧱 인공지능 기술의 발전 흐름

### 🧩 초기 인공지능의 탄생

* **1943년**: 매컬로-피츠 모델 → 인간 신경을 이진법으로 표현한 최초의 계산 모델
* **1958년**: 퍼셉트론(Perceptron) 등장 → 단층 신경망 개념 정립 (Frank Rosenblatt)

### 🧩 첫 번째 AI 겨울

* **1970년대**: 단층 퍼셉트론이 XOR 문제를 해결하지 못함 → 한계 인식 → 연구 침체

### 🧩 다층 신경망과 역전파

* **1986년**: MLP(Multi-layer Perceptron)와 Backpropagation 도입 → XOR 문제 해결

### 🧩 두 번째 AI 겨울

* **1990년대**: 심화된 모델에서 기울기 소실 문제가 발생 → 학습 난항

### 🧩 딥러닝의 부활

* **2006년**: Geoffrey Hinton의 Deep Belief Network, RBM을 통한 비지도 학습 제안 → 재조명
* **2012년**: AlexNet (GPU 2장으로 구성된 CNN 모델) → 이미지넷 대회 우승, 딥러닝 붐 유발

---

## 💡 핵심 기술과 개념 정리

### 🔹 GPU가 중요한 이유는?

* 행렬 연산 병렬 처리에 특화 → AI 학습 속도 획기적 향상
* 수천 개의 CUDA 코어를 활용한 연산 → CPU 대비 압도적 성능

### 🔹 딥러닝 vs 머신러닝 vs 인공지능

* **인공지능(AI)**: 인간의 사고, 판단, 학습 등을 모방
* **머신러닝(ML)**: 데이터 기반 학습 알고리즘을 개발하는 AI의 하위 분야
* **딥러닝(DL)**: 인공신경망 기반의 고도화된 머신러닝

---

## 💻 실무 연계 기술 고찰

### 1. 왜 파이썬이 AI에서 중심 언어인가?

* 직관적 문법, 수치/통계 라이브러리 풍부
* 비전문가도 쉽게 접근 가능 → 연구자 주도의 빠른 생태계 확장

### 2. GPT 및 LLM의 한계

* 보안 문제: 클라우드 기반 LLM은 구조상 유출 위험 존재
* 메타 인식 부족: 비정상적 질문에도 일관성 없는 답변 생성
* 대표 사례: ‘세종대왕 맥북 던짐’, ‘수양대군 트월킹’ 등의 우스꽝스러운 응답 사례

---

## 🔐 최신 트렌드: sLM(small Language Model)

### ✅ 등장 배경

* LLM은 크고 복잡하며 클라우드 기반 → 비용, 속도, 보안 이슈 발생
* 소형 모델(sLM)은 개인 서버에서도 구동 가능하며 경량화에 적합

### ✅ 시장성

* 2022년 7조원 → 2029년 23조원 이상 성장 전망
* 기업 내부 시스템에 쉽게 통합 가능

---

## 📂 챗봇 모델 구조 저장 방식

* **가중치 파일 (.bin)**: 모델 파라미터 저장
* **모델 구조 (.json)**: 레이어 구성, 연결 방식
* **설정 파일 (.json)**: 동작 설정, 환경 변수
* **전처리 정보 (.txt / .json)**: 토크나이저, normalization 규칙 등

---

## ✅ 탐구 결론 및 시사점

* 인공지능은 반복된 침체와 혁신을 거치며 기술적 돌파구를 마련해 왔다
* 생성형 AI의 급성장과 함께, 그 한계도 분명해졌고 새로운 대안(sLM)들이 떠오르고 있다
* 기술은 결국 ‘크기’가 아닌 ‘적합성’으로 진화할 것이라는 방향성 제시

---

> ⓒ 2025 | TA Partners 인공지능 인턴십 발표 정리
